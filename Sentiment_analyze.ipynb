{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHbJ4Kzu6yCYqlv4FAQb8e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hkd225/sentiment-anlyze/blob/main/Sentiment_analyze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 1. SETUP & IMPORTS\n",
        "# ===================================================================\n",
        "# Instalasi Library\n",
        "# !pip install scikit-learn transformers datasets torch pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# HuggingFace Transformers & Datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# Tentukan device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Menggunakan device: {device}\")\n",
        "\n",
        "# ===================================================================\n",
        "# 2. DATA LOADING & PREPROCESSING (MODIFIED)\n",
        "# ===================================================================\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('indonesian-adjective-sentiment-raw.csv')\n",
        "print(\"\\n--- Data Awal ---\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "### Fungsi untuk Labeling Sentiment Heuristik (DIREVISI) ###\n",
        "def determine_sentiment_from_explanation(explanation):\n",
        "    \"\"\"Tentukan sentiment berdasarkan kata kunci dalam penjelasan (Heuristic Labeling)\"\"\"\n",
        "    explanation_lower = str(explanation).lower()\n",
        "\n",
        "    positive_keywords = [\n",
        "        'baik', 'bagus', 'indah', 'senang', 'gembira', 'puas', 'suka', 'hebat',\n",
        "        'unggul', 'positif', 'menguntungkan', 'bermanfaat', 'menyenangkan',\n",
        "        'ceria', 'bahagia', 'nikmat', 'nyaman', 'mantap', 'luar biasa', 'cantik',\n",
        "        'elok', 'molek', 'menarik', 'memuaskan', 'sempurna', 'optimal', 'bagus',\n",
        "        'terbaik', 'sukses', 'berhasil', 'mengagumkan'\n",
        "    ]\n",
        "\n",
        "    negative_keywords = [\n",
        "        'buruk', 'jelek', 'tidak baik', 'susah', 'sulit', 'negatif', 'merugikan',\n",
        "        'menyedihkan', 'mengecewakan', 'berbahaya', 'menyakitkan', 'parah',\n",
        "        'rusak', 'gagal', 'celaka', 'sengsara', 'menderita', 'menyesal', 'malas',\n",
        "        'bodoh', 'kotor', 'jahat', 'menjijikkan', 'menakutkan', 'mengerikan',\n",
        "        'menjengkelkan', 'jelek', 'gagal'\n",
        "    ]\n",
        "\n",
        "    positive_count = sum(1 for keyword in positive_keywords if keyword in explanation_lower)\n",
        "    negative_count = sum(1 for keyword in negative_keywords if keyword in explanation_lower)\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        return 'positive'\n",
        "    elif negative_count > positive_count:\n",
        "        return 'negative'\n",
        "    elif positive_count == 0 and negative_count == 0:\n",
        "        # Hanya netral jika tidak ada keyword sentiment sama sekali\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        # Ambiguitas (count positif == count negatif > 0), hapus data ini (NaN)\n",
        "        return np.nan\n",
        "\n",
        "def create_training_text(row):\n",
        "    \"\"\"Gabungkan kata dan penjelasan untuk konteks yang lebih baik bagi model\"\"\"\n",
        "    word = str(row['word']).strip()\n",
        "    explanation = str(row['explanation']).strip()\n",
        "    return f\"Kata sifat '{word}' berarti {explanation}\"\n",
        "\n",
        "# Terapkan Labeling\n",
        "df['sentiment'] = df['explanation'].apply(determine_sentiment_from_explanation)\n",
        "df['training_text'] = df.apply(create_training_text, axis=1)\n",
        "\n",
        "# Map sentiment ke label numerik\n",
        "sentiment_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "df['label'] = df['sentiment'].map(sentiment_mapping)\n",
        "\n",
        "# Hapus rows yang tidak valid (termasuk NaN baru dari ambiguitas)\n",
        "df = df.dropna(subset=['label'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "print(\"\\n--- Distribusi Label Akhir ---\")\n",
        "print(df['sentiment'].value_counts())\n",
        "print(f\"Total sampel final: {len(df)}\")\n",
        "\n",
        "# ===================================================================\n",
        "# 3. CLASS WEIGHTS & DATA SPLIT\n",
        "# ===================================================================\n",
        "\n",
        "# Hitung Bobot Kelas (Class Weights)\n",
        "labels_unique = df['label'].unique()\n",
        "labels_unique.sort()\n",
        "\n",
        "try:\n",
        "    # Hitung bobot yang berbanding terbalik dengan frekuensi kelas\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=labels_unique,\n",
        "        y=df['label']\n",
        "    )\n",
        "except ValueError:\n",
        "    print(\"Warning: Tidak bisa menghitung bobot kelas, menggunakan bobot default (uniform).\")\n",
        "    class_weights = np.ones(len(labels_unique))\n",
        "\n",
        "# Konversi ke Tensor PyTorch\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(\"\\nBobot kelas yang dihitung (Baru):\", weights.tolist()) # Bobot akan berubah karena jumlah data netral berkurang\n",
        "\n",
        "# Split data menggunakan stratify\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
        "\n",
        "print(f\"\\n--- Split Dataset ---\")\n",
        "print(f\"Training: {len(train_df)} samples\")\n",
        "print(f\"Validation: {len(val_df)} samples\")\n",
        "print(f\"Test: {len(test_df)} samples\")\n",
        "\n",
        "# ===================================================================\n",
        "# 4. CUSTOM TRAINER DAN METRIK (Tidak Berubah)\n",
        "# ===================================================================\n",
        "\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    \"\"\"Trainer kustom dengan weighted loss.\"\"\"\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
        "                        labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"Fungsi untuk menghitung metrik F1-Score (weighted) dan Akurasi.\"\"\"\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "\n",
        "    f1 = f1_score(p.label_ids, preds, average='weighted', zero_division=0)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "\n",
        "    return {\"accuracy\": acc, \"f1_weighted\": f1}\n",
        "\n",
        "# ===================================================================\n",
        "# 5. MODEL SETUP & TOKENIZATION (Tidak Berubah)\n",
        "# ===================================================================\n",
        "\n",
        "model_name = \"indolem/indobert-base-uncased\"\n",
        "print(f\"\\nLoading tokenizer & model: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"training_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "print(\"Mempersiapkan dataset...\")\n",
        "train_dataset = HFDataset.from_pandas(train_df[['training_text', 'label']])\n",
        "val_dataset = HFDataset.from_pandas(val_df[['training_text', 'label']])\n",
        "test_dataset = HFDataset.from_pandas(test_df[['training_text', 'label']])\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "columns_to_keep = ['input_ids', 'attention_mask', 'label']\n",
        "tokenized_train = tokenized_train.remove_columns([col for col in tokenized_train.column_names if col not in columns_to_keep])\n",
        "tokenized_val = tokenized_val.remove_columns([col for col in tokenized_val.column_names if col not in columns_to_keep])\n",
        "tokenized_test = tokenized_test.remove_columns([col for col in tokenized_test.column_names if col not in columns_to_keep])\n",
        "\n",
        "print(\"Initializing standard model (IndoBERT)...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    id2label={0: \"negative\", 1: \"neutral\", 2: \"positive\"},\n",
        "    label2id={\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        ").to(device)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# ===================================================================\n",
        "# 6. TRAINING (MODIFIED HYPERPARAMETERS)\n",
        "# ===================================================================\n",
        "\n",
        "# TRAINING ARGUMENTS\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sentiment-model-output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=12, # DITINGKATKAN\n",
        "    per_device_train_batch_size=4, # DITURUNKAN\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50,\n",
        "    eval_steps=50,\n",
        "    save_steps=200,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1_weighted\",\n",
        "    greater_is_better=True,\n",
        "    learning_rate=1e-5, # DITURUNKAN\n",
        "    weight_decay=0.05, # DITINGKATKAN\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=None,\n",
        "    dataloader_pin_memory=True if torch.cuda.is_available() else False,\n",
        "    remove_unused_columns=False,\n",
        "    # Menambahkan save_total_limit untuk manajemen ruang disk\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "# Initialize Trainer dengan weighted loss\n",
        "trainer = WeightedLossTrainer(\n",
        "    class_weights=weights,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)], # DITINGKATKAN\n",
        ")\n",
        "\n",
        "# TRAINING MODEL\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"=== MEMULAI TRAINING DENGAN WEIGHTED LOSS (PARAM BARU) ===\")\n",
        "print(\"=\"*40)\n",
        "train_results = trainer.train()\n",
        "\n",
        "# ===================================================================\n",
        "# 7. EVALUASI DAN PENYIMPANAN (Tidak Berubah)\n",
        "# ===================================================================\n",
        "\n",
        "# EVALUASI\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"=== EVALUASI MODEL FINAL PADA TEST SET ===\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = accuracy_score(test_df['label'], preds)\n",
        "print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_df['label'], preds,\n",
        "                            target_names=['negative', 'neutral', 'positive'],\n",
        "                            zero_division=0))\n",
        "\n",
        "# SIMPAN MODEL TERBAIK\n",
        "print(\"\\n=== MENYIMPAN MODEL TERBAIK ===\")\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_save_path = f\"./trained-sentiment-model-{timestamp}\"\n",
        "\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"✓ Model disimpan di: {model_save_path}\")\n",
        "\n",
        "# ===================================================================\n",
        "# 8. PREDICTOR CLASS & UJI COBA (Tidak Berubah)\n",
        "# ===================================================================\n",
        "\n",
        "class IndonesianSentimentAnalyzer:\n",
        "    \"\"\"Kelas untuk menguji model yang sudah dilatih.\"\"\"\n",
        "    def __init__(self, model_path):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def predict_word(self, word, explanation=\"\"):\n",
        "        if explanation:\n",
        "            text = f\"Kata sifat '{word}' berarti {explanation}\"\n",
        "        else:\n",
        "            text = f\"Kata sifat '{word}'\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        probs = predictions.cpu().numpy()[0]\n",
        "        predicted_class = np.argmax(probs)\n",
        "\n",
        "        sentiment_map = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"}\n",
        "\n",
        "        return {\n",
        "            'word': word,\n",
        "            'sentiment': sentiment_map[predicted_class],\n",
        "            'confidence': float(probs[predicted_class]),\n",
        "            'probabilities': {\n",
        "                'negative': float(probs[0]),\n",
        "                'neutral': float(probs[1]),\n",
        "                'positive': float(probs[2])\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Uji coba model\n",
        "print(\"\\n=== TEST MODEL YANG SUDAH DILATIH ===\")\n",
        "analyzer = IndonesianSentimentAnalyzer(model_save_path)\n",
        "\n",
        "test_words = [\n",
        "    (\"bagus\", \"sangat baik dan memuaskan\"),\n",
        "    (\"jelek\", \"tidak baik dan buruk\"),\n",
        "    (\"cepat\", \"bergerak dengan laju tinggi\"),\n",
        "    (\"indah\", \"menyenangkan dipandang mata\"),\n",
        "    (\"malas\", \"tidak mau bekerja atau berusaha\"),\n",
        "    (\"sukses\", \"berhasil mencapai tujuan\")\n",
        "]\n",
        "\n",
        "print(\"\\nHasil prediksi dengan model terlatih:\")\n",
        "for word, explanation in test_words:\n",
        "    result = analyzer.predict_word(word, explanation)\n",
        "    print(f\"✦ {result['word']:<8}: {result['sentiment']:<9} (confidence: {result['confidence']:.3f})\")\n",
        "\n",
        "# ===================================================================\n",
        "# 9. EKSPOR UNTUK WEBSITE (Tidak Berubah)\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\n=== MENYIMPAN MODEL UNTUK WEBSITE ===\")\n",
        "\n",
        "website_model = {\n",
        "    'word_to_sentiment': {},\n",
        "    'word_to_confidence': {},\n",
        "    'metadata': {\n",
        "        'model_type': 'fine-tuned-bert-weighted',\n",
        "        'timestamp': timestamp,\n",
        "        'total_words': len(df),\n",
        "        'test_accuracy': float(accuracy),\n",
        "        'training_samples': len(train_df),\n",
        "        'model_name': model_name\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Memprediksi sentimen untuk semua kata...\")\n",
        "for idx, row in df.iterrows():\n",
        "    if idx % 1000 == 0 and idx != 0:\n",
        "        print(f\"Processing {idx}/{len(df)}...\")\n",
        "\n",
        "    result = analyzer.predict_word(row['word'], row['explanation'])\n",
        "    website_model['word_to_sentiment'][row['word']] = result['sentiment']\n",
        "    website_model['word_to_confidence'][row['word']] = result['confidence']\n",
        "\n",
        "# Simpan sebagai JSON\n",
        "website_model_filename = f'sentiment_model_{timestamp}.json'\n",
        "with open(website_model_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(website_model, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✓ Model untuk website disimpan sebagai: {website_model_filename}\")\n",
        "\n",
        "# Download file (Jika menggunakan Google Colab)\n",
        "# from google.colab import files\n",
        "# try:\n",
        "#     files.download(website_model_filename)\n",
        "# except:\n",
        "#     print(\"Download file diabaikan. Jalankan di Colab untuk mengaktifkan download.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ TRAINING & EVALUASI SELESAI!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "f_gmfJ9Rmcr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AlffMSSumy2Y"
      }
    }
  ]
}